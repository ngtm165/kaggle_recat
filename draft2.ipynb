{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phuocchung123/anaconda3/envs/GIN_korea/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv, os\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl.data.utils import split_dataset\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "from src_chung.model import reactionMPNN, training, inference\n",
    "from src_chung.dataset import GraphDataset\n",
    "from src_chung.util import collate_reaction_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GraphDataset('./data_chung/data_valid2.npz')\n",
    "valid_loader = DataLoader(\n",
    "        dataset=data,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_reaction_graphs,\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "for batch in valid_loader:\n",
    "    a=batch[0]\n",
    "    b=dgl.slice_batch(a,0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=4, num_edges=6,\n",
       "      ndata_schemes={'attr': Scheme(shape=(155,), dtype=torch.float32)}\n",
       "      edata_schemes={'edge_attr': Scheme(shape=(8,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'slice_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m b\u001b[38;5;241m=\u001b[39m\u001b[43mslice_batch\u001b[49m(data,\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'slice_batch' is not defined"
     ]
    }
   ],
   "source": [
    "b=slice_batch(data,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dim = valid_loader.dataset.rmol_node_attr[0].shape[1]\n",
    "edge_dim = valid_loader.dataset.rmol_edge_attr[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/workspace/kaggle_recat/src_chung/util.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  labels = torch.stack([torch.argmax(y) for y in torch.Tensor(batchdata[-1])], axis=0)\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_loader:\n",
    "    # if batch[-1].size()[0] != 32:\n",
    "    a=[b for b in batch[:23]]\n",
    "    b=[b for b in batch[23:23+6]]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phuocchung123/anaconda3/envs/GIN_korea/lib/python3.11/site-packages/dgl/core.py:82: DGLWarning: The input graph for the user-defined edge function does not contain valid edges\n",
      "  dgl_warning(\n"
     ]
    }
   ],
   "source": [
    "from src_chung.model import reactionMPNN\n",
    "import torch\n",
    "de=reactionMPNN(node_dim,edge_dim)\n",
    "x=torch.cat([de.mpnn(i) for i in a])\n",
    "# for mol in a:\n",
    "#     x.append(de.mpnn(mol))\n",
    "# len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1834, 300])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Suppose you have the following list of tensors\n",
    "list_of_tensors = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9])]\n",
    "\n",
    "# You can create a new tensor using torch.stack()\n",
    "new_tensor = torch.stack(list_of_tensors)\n",
    "\n",
    "print(new_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor=torch.Tensor()\n",
    "b_tensor=torch.cat((a_tensor,new_tensor))\n",
    "b_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tensor[0:new_tensor[0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [20., 21., 22.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your tensors\n",
    "tensor1 = torch.tensor([[1, 2, 3],[4,5,6],[20,21,22]])\n",
    "# tensor2 = torch.tensor([[10,11,12],[30,31,32]])\n",
    "tensor3 = torch.tensor([])\n",
    "print(tensor3.shape)\n",
    "# Expand dimensions of tensor2 and tensor3\n",
    "# tensor2 = tensor2.unsqueeze(0)\n",
    "# tensor3 = tensor3.unsqueeze(0)\n",
    "# print(tensor2.shape)\n",
    "\n",
    "# Now you can concatenate them\n",
    "result = torch.cat((tensor1, tensor3))\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2 = torch.tensor([[10,11,12],[30,31,32]])\n",
    "result_2=torch.cat((tensor2,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 7, 9]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.tensor([[1,2,3],[4,5,6]]),0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3916,  0.2927, -0.6941,  0.3057, -0.3279],\n",
      "        [-0.7313,  1.4835,  0.2858,  0.1565, -1.9621],\n",
      "        [-0.4837, -0.5856,  0.8011,  1.2600, -0.1051]], requires_grad=True)\n",
      "tensor([3, 4, 2])\n",
      "tensor([[ 1.4492,  0.7545, -0.0275, -0.7063,  1.0340],\n",
      "        [-0.5540, -0.2978,  0.4156,  0.8164, -1.0275],\n",
      "        [-1.1896,  0.4765,  0.9827,  0.6630,  1.4780]], requires_grad=True)\n",
      "tensor([[0.1577, 0.0583, 0.2158, 0.4900, 0.0782],\n",
      "        [0.3218, 0.0671, 0.2131, 0.3022, 0.0957],\n",
      "        [0.0972, 0.1144, 0.4426, 0.0993, 0.2465]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Example of target with class indices\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, emb_dim, dim_kqv):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.dim_kqv = dim_kqv\n",
    "        \n",
    "        self.wq = nn.Linear(emb_dim, dim_kqv)\n",
    "        self.wk = nn.Linear(emb_dim, dim_kqv)        \n",
    "        self.wv = nn.Linear(emb_dim, dim_kqv)\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        queries = self.wq(q)\n",
    "        keys = self.wk(k)\n",
    "        values = self.wv(v)\n",
    "        \n",
    "        score = queries.bmm(keys.transpose(1, 2))     \n",
    "          \n",
    "        score = torch.div(score, self.dim_kqv ** 0.5, rounding_mode='floor')\n",
    "        \n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        softmax = F.softmax(score, dim = -1)\n",
    "\n",
    "        return softmax.bmm(values)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim, dim_kqv):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(emb_dim, dim_kqv) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "        self.w0 = nn.Linear(num_heads * dim_kqv, emb_dim)\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        attentions = [h(q, k, v, mask) for h in self.heads]\n",
    "        attentions = torch.cat(attentions, dim = -1)\n",
    "        out = self.w0(attentions)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer, dimension, dropout):\n",
    "        super(Residual, self).__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors):\n",
    "        return self.dropout(self.norm(tensors[0] + self.sublayer(*tensors)))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim, ff_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, emb_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, residual_out):\n",
    "        return self.network(residual_out)\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ff_dim, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.dim_kqv = emb_dim // num_heads\n",
    "        \n",
    "        assert (self.dim_kqv * num_heads == emb_dim), \"Embedding size must be divisible by number of heads\" \n",
    "        \n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, emb_dim, self.dim_kqv),\n",
    "            dimension=emb_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "    \n",
    "        self.feed_forward = Residual(\n",
    "            FeedForward(emb_dim, ff_dim),\n",
    "            dimension=emb_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        src = self.attention(src, src, src, mask)\n",
    "        out = self.feed_forward(src)\n",
    "        return out\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 emb_dim, \n",
    "                 num_heads, \n",
    "                 ff_dim, \n",
    "                 num_layers, \n",
    "                 src_vocab_size,\n",
    "                 padding_index,\n",
    "                 dropout):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(emb_dim,\n",
    "                         num_heads,\n",
    "                         ff_dim,\n",
    "                         dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.embedding = nn.Embedding(src_vocab_size, emb_dim, padding_idx=0)\n",
    "        self.pe = PositionalEncoder(emb_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src)\n",
    "        \n",
    "        src = self.pe(src)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, None)\n",
    "            \n",
    "        return src\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ff_dim, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.dim_kqv = emb_dim // num_heads\n",
    "        \n",
    "        assert (self.dim_kqv * num_heads == emb_dim), \"Embedding size must be divisible by number of heads\"\n",
    "        \n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadAttention(num_heads, emb_dim, self.dim_kqv),\n",
    "            dimension=emb_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadAttention(num_heads, emb_dim, self.dim_kqv),\n",
    "            dimension=emb_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.feed_forward = Residual(\n",
    "            FeedForward(emb_dim, ff_dim),\n",
    "            dimension=emb_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, trg, memory, mask):\n",
    "        query = self.attention_1(trg, trg, trg, mask)\n",
    "        attentions = self.attention_2(query, memory, memory, None)\n",
    "        out = self.feed_forward(attentions)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 emb_dim, \n",
    "                 num_heads, \n",
    "                 ff_dim, \n",
    "                 num_layers, \n",
    "                 out_size, \n",
    "                 padding_index,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(emb_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.embedding = nn.Embedding(out_size, emb_dim, padding_idx=padding_index)\n",
    "        self.pe = PositionalEncoder(emb_dim)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, seq_len = trg.shape[0], trg.shape[1]\n",
    "        mask = torch.tril(torch.ones(batch_size, seq_len, seq_len))\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, trg, encoder_out):\n",
    "        trg = self.embedding(trg)\n",
    "        \n",
    "        trg = self.pe(trg)\n",
    "\n",
    "        mask = self.make_trg_mask(trg).to(trg.get_device())\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, encoder_out, mask)\n",
    "            \n",
    "        # return self.lin(trg)\n",
    "        return trg\n",
    "\n",
    "class VanillaTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 emb_dim, \n",
    "                 num_heads, \n",
    "                 ff_dim, \n",
    "                 num_layers, \n",
    "                 src_vocab_size, \n",
    "                 trg_vocab_size,\n",
    "                 device,\n",
    "                 padding_index,\n",
    "                 dropout):\n",
    "        super(VanillaTransformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(emb_dim, \n",
    "                               num_heads, \n",
    "                               ff_dim,\n",
    "                               num_layers, \n",
    "                               src_vocab_size,\n",
    "                               padding_index,\n",
    "                               dropout).to(device)\n",
    "        \n",
    "        self.decoder = Decoder(emb_dim,\n",
    "                               num_heads,\n",
    "                               ff_dim, \n",
    "                               num_layers,\n",
    "                               trg_vocab_size,\n",
    "                               padding_index,\n",
    "                               dropout).to(device)\n",
    "\n",
    "        self.lin = nn.Linear(emb_dim, trg_vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        encoder_out = self.encoder(src)\n",
    "\n",
    "        decoder_out = self.decoder(trg, encoder_out)\n",
    "        \n",
    "        out = self.lin(decoder_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaTransformer(EMBEDDING_DIM = 256,\n",
    "                           NUM_HEADS = 8,\n",
    "                           FF_DIM = 2048,\n",
    "                           NUM_LAYERS = 6,\n",
    "                           SOURCE_VOCAB_SIZE = 10,\n",
    "                           TARGET_VOCAB_SIZE=10,\n",
    "                           device,\n",
    "                           padding_index = 0,\n",
    "                           DROPOUT= 0.1).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=padding_index)\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS), position = 0, leave = True):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src = batch['x_source']\n",
    "        trg = batch['y_target']\n",
    "        \n",
    "        y_pred = model(src.to(device), trg[:, :-1].to(device))\n",
    "        y_pred = y_pred.reshape(-1, y_pred.shape[2])\n",
    "        \n",
    "        loss = loss_func(y_pred, trg[:, 1:].reshape(-1).to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch == 0 or (epoch + 1) % PRINT_EVERY == 0:\n",
    "      print('Epoch: {:<2} Train loss: {:0.4f}'.format(epoch + 1 , running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9584285714285713\n",
      "0.0032888184094918135\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list1=[0.951,0.961,0.959,0.958,0.959,0.959,0.962]\n",
    "\n",
    "print(np.mean(list1))\n",
    "print(np.std(list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595714285714284\n",
      "0.003288818409491814\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list2=[0.954,0.964,0.958,0.958,0.960,0.964,0.959]\n",
    "\n",
    "print(np.mean(list2))\n",
    "print(np.std(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9628571428571429\n",
      "0.0018844151368961328\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list3=[0.960,0.964,0.963,0.961,0.962,0.966,0.964]\n",
    "\n",
    "print(np.mean(list3))\n",
    "print(np.std(list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9604285714285713\n",
      "0.005996597674804393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list4=[0.969,0.956,0.965,0.952,0.967,0.957,0.957]\n",
    "\n",
    "print(np.mean(list4))\n",
    "print(np.std(list4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9664285714285714\n",
      "0.0016781914463529632\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list5=[0.967,0.963,0.965,0.967,0.968,0.967,0.968]\n",
    "\n",
    "print(np.mean(list5))\n",
    "print(np.std(list5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4515, 0.8778, 0.8910, 0.8566, 0.7635])\n",
      "tensor(0.8910)\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_a=torch.rand(5)\n",
    "print(tensor_a)\n",
    "print(tensor_a.max())\n",
    "print(tensor_a.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7524, 0.0887, 0.6099, 0.8253, 0.1242])\n",
      "tensor([0.8679])\n",
      "tensor([0.4912, 0.7736, 0.8539, 0.8524, 0.6790])\n"
     ]
    }
   ],
   "source": [
    "tensor_b=torch.rand(5)\n",
    "print(tensor_b)\n",
    "weight=0.5*torch.rand(1)+0.5\n",
    "# weight=weight.item()\n",
    "print(weight)\n",
    "tensor_sum=weight*tensor_a + (1-weight)*tensor_b\n",
    "print(tensor_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GIN_korea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
