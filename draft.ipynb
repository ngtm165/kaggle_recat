{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phuocchung123/anaconda3/envs/GIN_korea/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv, os\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl.data.utils import split_dataset\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "from src_chung.model import reactionMPNN, training, inference\n",
    "from src_chung.dataset import GraphDataset\n",
    "from src_chung.util import collate_reaction_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GraphDataset('./data_chung/data_valid.npz')\n",
    "valid_loader = DataLoader(\n",
    "        dataset=data,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_reaction_graphs,\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_loader:\n",
    "    if batch[-1].size()[0] != 32:\n",
    "        print(batch[-1].size())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {35: 800, 41: 800, 13: 800, 4: 800, 12: 800, 26: 800, 24: 800, 27: 800, 0: 800, 2: 800, 22: 800, 7: 800, 23: 800, 32: 800, 14: 800, 19: 800, 25: 800, 36: 800, 37: 800, 17: 800, 6: 800, 1: 800, 20: 800, 40: 800, 49: 800, 3: 800, 28: 800, 33: 800, 34: 800, 10: 800, 15: 800, 48: 800, 38: 800, 5: 800, 16: 800, 30: 800, 18: 800, 39: 800, 11: 800, 44: 800, 8: 800, 31: 800, 9: 800, 45: 800, 46: 800, 47: 800, 43: 800, 21: 800, 42: 800, 29: 800})\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "element_count = defaultdict(int)\n",
    "\n",
    "for element in a:\n",
    "    element_count[element] += 1\n",
    "\n",
    "print(element_count)\n",
    "print(len(element_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    a=batch[-1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Generate two random numbers between 0 and 1\n",
    "random_numbers = torch.rand(2)\n",
    "\n",
    "# Since we're assuming a uniform distribution between 0 and 1 for each number,\n",
    "# the probability of each number is simply 1 (the length of the interval).\n",
    "probabilities = torch.ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number 1: 0.16787117719650269, Percentage: 19.223329730622574%\n",
      "Number 2: 0.7053967714309692, Percentage: 80.77667026937743%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generate two random numbers\n",
    "num1 = torch.rand(1).item()\n",
    "num2 = torch.rand(1).item()\n",
    "\n",
    "# Calculate the total\n",
    "total = num1 + num2\n",
    "\n",
    "# Calculate the percentage of each number\n",
    "weight1 = (num1 / total) * 100\n",
    "weight2 = (num2 / total) * 100\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "net,\n",
    "train_loader,\n",
    "val_loader,\n",
    "model_path,\n",
    "val_monitor_epoch=1,\n",
    "n_forward_pass=5,\n",
    "cuda=torch.device('cuda:0'),\n",
    "):\n",
    "train_size = train_loader.dataset.__len__()\n",
    "batch_size = train_loader.batch_size\n",
    "nt_xent_criterion = NTXentLoss(cuda, batch_size)\n",
    "\n",
    "try:\n",
    "    rmol_max_cnt = train_loader.dataset.dataset.rmol_max_cnt\n",
    "    pmol_max_cnt = train_loader.dataset.dataset.pmol_max_cnt\n",
    "\n",
    "except:\n",
    "    rmol_max_cnt = train_loader.dataset.rmol_max_cnt\n",
    "    pmol_max_cnt = train_loader.dataset.pmol_max_cnt\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 20\n",
    "optimizer = Adam(net.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "\n",
    "# lr_scheduler = MultiStepLR(\n",
    "#     optimizer, milestones=[400, 450], gamma=0.1, verbose=False\n",
    "# )\n",
    "\n",
    "train_loss_all=[]\n",
    "val_loss_all=[]\n",
    "acc_all=[]\n",
    "acc_all_val=[]\n",
    "mcc_all=[]\n",
    "mcc_all_val=[]\n",
    "\n",
    "best_val_loss =1e10\n",
    "best_loss=1e10\n",
    "for epoch in range(n_epochs):\n",
    "    # training\n",
    "    net.train()\n",
    "    start_time = time.time()\n",
    "    train_loss_contra_list = []\n",
    "    \n",
    "    for batchdata in tqdm(train_loader, desc='Training_contra'):\n",
    "        inputs_rmol = [b.to(cuda) for b in batchdata[:rmol_max_cnt]]\n",
    "        inputs_pmol = [\n",
    "            b.to(cuda)\n",
    "            for b in batchdata[rmol_max_cnt : rmol_max_cnt + pmol_max_cnt]\n",
    "        ]\n",
    "        r_rep,p_rep= net(inputs_rmol, inputs_pmol)\n",
    "        loss_sc=nt_xent_criterion(r_rep, p_rep)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_contra = loss.detach().item()\n",
    "        train_loss_contra_list.append(train_loss_contra)\n",
    "\n",
    "    print(\"--- training epoch %d, loss %.3f, time elapsed(min) %.2f---\"\n",
    "          % (epoch, np.mean(train_loss_contra_list), (time.time() - start_time) / 60))\n",
    "    \n",
    "\n",
    "    if np.mean(val_loss_list) < best_loss:\n",
    "        best_loss = np.mean(val_loss_list)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "\n",
    "\n",
    "\n",
    "    for batchdata in tqdm(train_loader, desc='Training'):\n",
    "        inputs_rmol = [b.to(cuda) for b in batchdata[:rmol_max_cnt]]\n",
    "        inputs_pmol = [\n",
    "            b.to(cuda)\n",
    "            for b in batchdata[rmol_max_cnt : rmol_max_cnt + pmol_max_cnt]\n",
    "        ]\n",
    "\n",
    "        labels = batchdata[-1]\n",
    "        targets.extend(labels.tolist())\n",
    "        labels = labels.to(cuda)\n",
    "\n",
    "        r_rep,p_rep= net(inputs_rmol, inputs_pmol)\n",
    "        loss_sc=nt_xent_criterion(r_rep, p_rep)\n",
    "\n",
    "        pred = net.predict(torch.sub(r_rep,p_rep))\n",
    "        preds.extend(torch.argmax(pred, dim=1).tolist())\n",
    "        loss_ce = loss_fn(pred, labels)\n",
    "        loss=weight1*loss_ce+weight2*loss_sc\n",
    "\n",
    "        ##Uncertainty \n",
    "        # loss = (1 - 0.1) * loss.mean() + 0.1 * (\n",
    "        #     loss * torch.exp(-logvar) + logvar\n",
    "        # ).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = loss.detach().item()\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    mcc = matthews_corrcoef(targets, preds)\n",
    "    train_loss_all.append(np.mean(train_loss_list))\n",
    "    acc_all.append(acc)\n",
    "    mcc_all.append(mcc)\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "\n",
    "        \n",
    "        print(\n",
    "            \"--- training epoch %d, loss %.3f, acc %.3f, mcc %.3f, time elapsed(min) %.2f, weight1 %.3f, weight2 %.3f---\"\n",
    "            % (\n",
    "                epoch,\n",
    "                np.mean(train_loss_list),\n",
    "                acc,\n",
    "                mcc,\n",
    "                (time.time() - start_time) / 60,\n",
    "                weight1,\n",
    "                weight2,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # lr_scheduler.step()\n",
    "\n",
    "    # validation with test set\n",
    "    if val_loader is not None and (epoch + 1) % val_monitor_epoch == 0:\n",
    "\n",
    "        batch_size = val_loader.batch_size\n",
    "\n",
    "        try:\n",
    "            rmol_max_cnt = val_loader.dataset.dataset.rmol_max_cnt\n",
    "            pmol_max_cnt = val_loader.dataset.dataset.pmol_max_cnt\n",
    "\n",
    "        except:\n",
    "            rmol_max_cnt = val_loader.dataset.rmol_max_cnt\n",
    "            pmol_max_cnt = val_loader.dataset.pmol_max_cnt\n",
    "\n",
    "        net.eval()\n",
    "        val_loss_list=[]\n",
    "        val_targets=[]\n",
    "        val_preds=[]\n",
    "\n",
    "        # MC_dropout(net)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batchdata in tqdm(val_loader, desc='Validating'):\n",
    "                inputs_rmol = [b.to(cuda) for b in batchdata[:rmol_max_cnt]]\n",
    "                inputs_pmol = [\n",
    "                    b.to(cuda)\n",
    "                    for b in batchdata[rmol_max_cnt : rmol_max_cnt + pmol_max_cnt]\n",
    "                ]\n",
    "\n",
    "                labels_val = batchdata[-1]\n",
    "                val_targets.extend(labels_val.tolist())\n",
    "                labels_val = labels_val.to(cuda)\n",
    "\n",
    "\n",
    "                r_rep,p_rep=net(inputs_rmol, inputs_pmol)\n",
    "                loss_sc=nt_xent_criterion(r_rep, p_rep)\n",
    "                pred_val = net.predict(torch.sub(r_rep,p_rep))\n",
    "                val_preds.extend(torch.argmax(pred_val, dim=1).tolist())    \n",
    "                loss_ce=loss_fn(pred_val,labels_val)\n",
    "\n",
    "                loss=weight1*loss_ce+weight2*loss_sc\n",
    "\n",
    "                val_loss = loss.item()\n",
    "                val_loss_list.append(val_loss)\n",
    "\n",
    "            if np.mean(val_loss_list) < best_val_loss:\n",
    "                best_val_loss = np.mean(val_loss_list)\n",
    "                torch.save(net.state_dict(), model_path)\n",
    "\n",
    "            val_acc = accuracy_score(val_targets, val_preds)\n",
    "            val_mcc = matthews_corrcoef(val_targets, val_preds)\n",
    "\n",
    "\n",
    "            val_loss_all.append(np.mean(val_loss_list))\n",
    "            acc_all_val.append(val_acc)\n",
    "            mcc_all_val.append(val_mcc)\n",
    "\n",
    "\n",
    "\n",
    "            print(\n",
    "                \"--- validation at epoch %d, val_loss %.3f, val_acc %.3f, val_mcc %.3f ---\"\n",
    "                % (epoch, np.mean(val_loss_list),val_acc,val_mcc)\n",
    "            )\n",
    "            print('\\n'+'*'*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GIN_korea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
